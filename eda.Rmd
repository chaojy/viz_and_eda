---
title: "Exploratory Analysis"
output: github_document
---

```{r setup}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Load the weather data

```{r}
weather_df =
  rnoaa::meteo_pull_monitors(
    c("USW00094728", "USC00519397", "USS0023B17S"),
    var = c("PRCP", "TMIN", "TMAX"),
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>% 
  mutate(
    name = recode(
      id,
      USW00094728 = "CentralPark_NY",
      USC00519397 = "Waikiki_HA",
      USS0023B17S = "Waterhole_WA"),
    tmin = tmin / 10,
    tmax = tmax / 10,
    month = lubridate::floor_date(date, unit = "month")) %>% 
  select(name, id, everything())

weather_df
```

lubridate::floor_date   code rounds month and date to the lowest.

## `group_by`

```{r}
weather_df %>% 
  group_by(name)
```

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  ungroup()
```

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  ungroup(month)
```

## counting things

count month observations
```{r}
weather_df %>% 
  group_by(month) %>% 
  summarize(n_obs = n())
```

```{r}
weather_df %>% 
  group_by(name) %>% 
  summarize(n_obs = n())
```

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarize(n_obs = n())
```

we can use `count()` as well - will generate the same thing as group_by()

```{r}
weather_df %>% 
  count(month, name = "n_obs")
```

```{r}
weather_df %>% 
  count(name, month, name = "n_obs")
```

**Never** use base R's `table`

```{r, eval = FALSE}
weather_df %>% 
  pull(month) %>% 
  table()
```

Why?
  Because the result of this is not a dataframe.  Not useable for later purpose
  
Other helpful counters

```{r}
weather_df %>% 
  group_by(month) %>% 
  summarize(
    n_obs = n(),
    n_days = n_distinct(date)
    )
```
This above code tells me the number of observations in the grouping specificed (month), AND it also tells us how many of those dates are distinct.

## A digression on 2x2 tables

```{r}
weather_df %>% 
  filter(name != "Waikiki_HA") %>% 
  mutate(
    cold = case_when(
      tmax < 5 ~ "cold",
      tmax >= 5 ~ "not cold",
      TRUE      ~ ""
    )
  ) %>% 
  group_by(name, cold) %>% 
  summarize(count = n())
```

```{r}
weather_df %>% 
  filter(name != "Waikiki_HA") %>% 
  mutate(
    cold = case_when(
      tmax < 5 ~ "cold",
      tmax >= 5 ~ "not cold",
      TRUE      ~ ""
    )
  ) %>% 
  group_by(name, cold) %>% 
  summarize(count = n())
```

```{r}
weather_df %>% 
  filter(name != "Waikiki_HA") %>% 
  mutate(
    cold = case_when(
      tmax < 5 ~ "cold",
      tmax >= 5 ~ "not cold",
      TRUE      ~ ""
    )
  ) %>% 
janitor::tabyl(name, cold)
```


## General summaries

```{r}
weather_df %>% 
  group_by(month) %>% 
  summarize(
    mean_tmax = mean(tmax)
  )
```
call a new variable whatever as the mean variable equal to the function and the column name within ().
so there are some NA values in May and July.  This because, if you go into viewer, you will see some missing data.  Solution is to drop NA

```{r}
weather_df %>% 
  group_by(month) %>% 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE),
    mean_prcp = mean(prcp, na.rm = TRUE),
    median_tmin = median(tmin, na.rm = TRUE)
  )
```

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE),
    mean_prcp = mean(prcp, na.rm = TRUE),
    median_tmin = median(tmin, na.rm = TRUE)
  )
```
You can do lots of summaries, as shown above.

This is a dataframe! So we can make a plot.

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE),
    mean_prcp = mean(prcp, na.rm = TRUE),
    median_tmin = median(tmin, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = month, y = mean_tmax, color = name)) +
  geom_point() +
  geom_line()
```
So you can pipe directly into ggplot.
you can filter, mutate, etc.

Suppose you want to summarize many columns.

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarize(across(prcp:tmin, mean))
```

Reminder: sometimes your results are easier to read in another format.

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarize(mean_tmax = mean(tmax)) %>% 
  pivot_wider(
    names_from = name,
    values_from = mean_tmax
  ) %>% 
  knitr::kable(digits = 1)
```

## group_by and mutate
mutate also respects new grouping structure - beware because can cause problems if the grouping is unrecognized

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>% view()
```

you may want to center your groups (group first, then mutate so that it works one group at a time) and center the data:

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    mean_tmax = mean(tmax, na.rm = TRUE),
    centered_tmax = tmax - mean_tmax
  ) %>%
  ggplot(aes(x = date, y = centered_tmax, color = name)) +
  geom_point()
```

what about window functions
in the preceding, you have repeated mean_tmax for each row.

ranking ..

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  mutate(temp_rank = min_rank(tmax)) %>% 
  filter(temp_rank == 1) %>% 
  view()
```

min_rank function gives you the lowest rank (#1) for the variable you specify

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  mutate(temp_rank = min_rank(desc(tmax))) %>% 
  filter(temp_rank == 1) %>% 
  view()
```
min_rank(desc(tmax)) allows you to calculate from the bottom up.

lag.  take a bunch of variables (so row values), copy and shift over, and descend.

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    lag_temp = lag(tmax)
  )
```

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    lag_temp = lag(tmax, 5)
  )
```
lag by 5.

lag allows you to compare today's values with some specified prior day's values (like means, etc.)

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(temp_change = tmax - lag(tmax)) %>% 
  summarize(
    temp_change_max = max(temp_change, na.rm = TRUE),
    temp_change_sd = sd(temp_change, na.rm = TRUE)
  )
```
take home point: we answered the question: can you compute the day to day temperature variability and standard deviation for each of the weather station over the course of the year.  Sounds difficult, but conceptually, this is how yuo would do it in 6 lines of code!
From coding perspective, answering a relatively complex question in a way that is clear and coherent in a pretty short sequence of code.  Pat yourself on the back.


## Quick note

summarize only gets you so far.  powerful.  but some questions requiring some analyses cannot be solved with summarize.  but for now, summarize works.  someday, we will have to do something more complicated.  but this is very good for now.

BIG SUMMARY SO FAR
naming code/variables in files
r markdown
using git and git hub
reading in data
manipulating data
tidying data
making plots and customizing it
exploratory analyses
covering a lot of ground
takes time to congeal

Learning assessments #1

PULSE data

```{r}
library(haven)

pulse_df = read_sas("./data/public_pulse_data.sas7bdat") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    bdi_score_bl:bdi_score_12m,
    names_to = "visit",
    names_prefix = "bdi_score_",
    values_to = "bdi"
  ) %>% 
  mutate(
    visit = replace(visit, visit == "bl", "00m"),
    visit = factor(visit, levels = str_c(c("00", "01", "06", "12"), "m"))) %>%
  arrange(id, visit)
  
pulse_df %>% 
  group_by(visit) %>% 
  summarize(
    mean_bdi = mean(bdi, na.rm = TRUE),
    median_bdi = median(bdi, na.rm = TRUE)
  ) %>% 
 knitr::kable(digits = 2)
```

Jeff's code - let's see how the output is similar or different from my code

```{r}
pulse_jeff = 
  haven::read_sas("./data/public_pulse_data.sas7bdat") %>%
  janitor::clean_names() %>%
  pivot_longer(
    bdi_score_bl:bdi_score_12m,
    names_to = "visit", 
    names_prefix = "bdi_score_",
    values_to = "bdi") %>%
  select(id, visit, everything()) %>%
  mutate(
    visit = replace(visit, visit == "bl", "00m"),
    visit = factor(visit, levels = str_c(c("00", "01", "06", "12"), "m"))) %>%
  arrange(id, visit)

pulse_jeff %>% 
  group_by(visit) %>% 
  summarize(
    mean_bdi = mean(bdi, na.rm = TRUE),
    median_bdi = median(bdi, na.rm = TRUE)) %>% 
  knitr::kable(digits = 3)
```
This quick summary suggests a relatively large drop in the typical BDI score from baseline to 1 month, with small or no changes thereafter.

bl should be cahnged to 00m because otherwise, it comes at end of the table and doesn't make sense.
also, he recoded the visit variable to factor, which will allow for more manipulations later on.

Learning assessments #2

FAS data

```{r}
fas_litters =
  read_csv("./data/FAS_litters.csv") %>% 
  janitor::clean_names() %>% 
  relocate(litter_number) %>% 
  separate(group, into = c("dose", "day_of_tx"), sep = 3)

fas_pups =
  read_csv("./data/FAS_pups.csv") %>% 
  janitor::clean_names()

fas_join =
  left_join(fas_pups, fas_litters, by = "litter_number") %>% 
  relocate(litter_number, dose, day_of_tx) %>% 
  group_by(dose, day_of_tx) %>% 
  summarize(
    mean_pivot = mean(pd_pivot, na.rm = TRUE),
    median_pivot = median(pd_pivot, na.rm = TRUE) 
  ) %>% 
  knitr::kable(digits = 3)
```

Jeff's code.  Let's see how it differs and learn why

```{r}
pup_jeff = 
  read_csv("./data/FAS_pups.csv") %>%
  janitor::clean_names() %>%
  mutate(sex = recode(sex, `1` = "male", `2` = "female")) 

litter_jeff = 
  read_csv("./data/FAS_litters.csv") %>%
  janitor::clean_names() %>%
  separate(group, into = c("dose", "day_of_tx"), sep = 3)

fas_jeff = left_join(pup_jeff, litter_jeff, by = "litter_number") 

fas_jeff %>% 
  group_by(dose, day_of_tx) %>% 
  drop_na(dose) %>% 
  summarize(mean_pivot = mean(pd_pivot, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = dose, 
    values_from = mean_pivot) %>% 
  knitr::kable(digits = 3)
```

in Jeff's code, he just looked at mean values for mean_pivot.
he also pivoted wider so dose is in the columns in wide format.
let's try this code to understand it better

```{r}
fas_join =
  left_join(fas_pups, fas_litters, by = "litter_number") %>% 
  relocate(litter_number, dose, day_of_tx) %>% 
  group_by(dose, day_of_tx) %>%
  drop_na(dose) %>% 
  summarize(
    mean_pivot = mean(pd_pivot, na.rm = TRUE)
  ) %>% 
  pivot_wider(
    names_from = dose,
    values_from = mean_pivot
  ) %>% 
  knitr::kable(digits = 3)
```
he condensed a lot of stuff.  got rid of NAs, pivoted to wider.

"Note: In both of these examples, the data are structure such that repeated observations are made on the same study units. In the PULSE data, repeated observations are made on subjects over time; in the FAS data, pups are “repeated observations” within litters. The analyses here, and plots made previously, are exploratory – any more substantial claims would require appropriate statistical analysis for non-independent samples."

